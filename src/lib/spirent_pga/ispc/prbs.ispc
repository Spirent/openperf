#include "inet.h"

/*
 * The masks for the various prbs_xorshift_nextN() and prbs_xorshift_stepN()
 * functions below are generated from the companion matrix of the
 * characteristic polynomial for our PRBS sequence, x^23 + x^18 + 1.
 *
 * See https://en.wikipedia.org/wiki/Linear-feedback_shift_register#Matrix_forms
 * for details.
 *
 * Note that since we are dealing with big-endian hardware, we adjust the matrix form
 * slightly to...
 *
 * | c_n-1 c_n-2 c_n-3 ... c_0 |
 * |     1     0     0 ...   0 |
 * |     0     1     0 ...   0 |
 * |     .     .     . .     . |
 * |     .     .     .  .    . |
 * |     .     .     .   .   . |
 * |     0     0     0 ...   0 |
 *
 * So, given the above matrix M, we can generate a matrix, M^n, that specifies
 * which bits of the initial value are utilized when computing the n'th
 * step of the shift register sequence.
 *
 * Each row, r, of the resulting matrix contains the generative bits for
 * the n'th step of the sequence when the initial seed value is shifted r
 * bits to the left, e.g. each row is a mask.
 *
 * This leads to the 2-d arrays below, where each column represents a
 * 32 x 32 matrix for the corresponding SIMD lane and each row represents
 * a mask to apply to the rotated seed value.
 *
 * The next value in the sequence is generated by XOR'ing all of the
 * masked and rotated seed values together.
 *
 * Finally, note that all 2-d array rows where all values are 0 have been
 * dropped.
 */

#define PRBS_SEED_MASK 0x7fffff

inline unsigned int32 prbs_rotate_left(unsigned int32 value, uniform unsigned int count)
{
    return ((value << 23 | value) << count | value >> (23 - count));
}

#if TARGET_WIDTH == 4

inline unsigned int32 prbs_next(uniform unsigned int32 seed, int index)
{
    static const uniform unsigned int32 masks[][4] = {
        { 0x00000000,0x0003ffff,0x00000000,0x0003ffff },
        { 0x00000000,0x00000000,0xffffffff,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x000fffff },
        { 0x00000000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0xffffffff,0x00000000 },
        { 0x00000000,0xffffffff,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00fffffe,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000003 },
        { 0x00000000,0x00000000,0x00000000,0x00000000 },
        { 0xffffffff,0x00000000,0xf800000f,0x00000000 },
        { 0x00000000,0x0fffffe0,0x00000000,0xffffffff },
        { 0x00000000,0x00000000,0x0000003f,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0xffffffff },
        { 0xfffffe00,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x000003ff,0x00000000,0x000003ff },
        { 0x00000000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0xffffffff,0x00000000,0xffffe000 },
        { 0x00003fff,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0xffffffff }
    };

    uniform unsigned int32 base = seed & PRBS_SEED_MASK;  /* only need the lower 23 bits of the seed value */

    return ((base & masks[0][index])
            ^ (prbs_rotate_left(base, 1) & masks[1][index])
            ^ (prbs_rotate_left(base, 2) & masks[2][index])
            ^ (prbs_rotate_left(base, 4) & masks[4][index])
            ^ (prbs_rotate_left(base, 5) & masks[5][index])
            ^ (prbs_rotate_left(base, 6) & masks[6][index])
            ^ (prbs_rotate_left(base, 7) & masks[7][index])
            ^ (prbs_rotate_left(base, 9) & masks[9][index])
            ^ (prbs_rotate_left(base, 10) & masks[10][index])
            ^ (prbs_rotate_left(base, 11) & masks[11][index])
            ^ (prbs_rotate_left(base, 13) & masks[13][index])
            ^ (prbs_rotate_left(base, 14) & masks[14][index])
            ^ (prbs_rotate_left(base, 15) & masks[15][index])
            ^ (prbs_rotate_left(base, 18) & masks[18][index])
            ^ (prbs_rotate_left(base, 19) & masks[19][index])
            ^ (prbs_rotate_left(base, 20) & masks[20][index]));
}

inline unsigned int32 prbs_step(unsigned int32 seed)
{
    static const uniform unsigned int32 masks[] = {
        0x0003ffff,
        0x000fffff,
        0x00000003,
        0xffffffff,
        0xffffffff,
        0x000003ff,
        0xffffe000,
        0xffffffff
    };

    unsigned int32 base = seed & PRBS_SEED_MASK;  /* only need the lower 23 bits of the seed value */

    return ((base & masks[0])
            ^ (prbs_rotate_left(base,  2) & masks[1])
            ^ (prbs_rotate_left(base,  7) & masks[2])
            ^ (prbs_rotate_left(base, 10) & masks[3])
            ^ (prbs_rotate_left(base, 13) & masks[4])
            ^ (prbs_rotate_left(base, 15) & masks[5])
            ^ (prbs_rotate_left(base, 18) & masks[6])
            ^ (prbs_rotate_left(base, 20) & masks[7]));
}

#elif TARGET_WIDTH == 8

inline unsigned int32 prbs_next(uniform unsigned int32 seed, int index)
{
    static const uniform unsigned int32 masks[][8] = {
        { 0x00000000,0x0003ffff,0x00000000,0x0003ffff,0x00000000,0x00000000,0x00000000,0x0003ffff },
        { 0x00000000,0x00000000,0xffffffff,0x00000000,0xfff80000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x000fffff,0x00000000,0xffffffff,0x00000000,0x000fffff },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0xffe00000,0xffffffff },
        { 0x00000000,0x00000000,0xffffffff,0x00000000,0x003fffff,0x00000000,0x003fffff,0x003fffff },
        { 0x00000000,0xffffffff,0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00fffffe,0x00000000,0xff000000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000003,0x00000000,0xfe000003,0x00000000,0x00000003 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0xffffffff,0x03ffffff,0x03fffff8 },
        { 0xffffffff,0x00000000,0xf800000f,0x00000000,0xfffffff0,0x00000000,0x0000000f,0x0000000f },
        { 0x00000000,0x0fffffe0,0x00000000,0xffffffff,0x00000000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x0000003f,0x00000000,0xe0000000,0x00000000,0xffffffff,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0x3fffffff,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0xffffffff,0x00000000,0x800000ff,0x000000ff,0xffffff00 },
        { 0xfffffe00,0x00000000,0x00000000,0x00000000,0xfffffe00,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x000003ff,0x00000000,0x000003ff,0x00000000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0xffffffff,0x00000000,0xfffff800,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0x00000fff,0xffffffff,0xffffffff },
        { 0x00000000,0xffffffff,0x00000000,0xffffe000,0x00000000,0x00000000,0x00000000,0xffffe000 },
        { 0x00003fff,0x00000000,0x00000000,0x00000000,0xffffc000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0xffffffff,0x00000000,0x00000000,0x00000000,0xffffffff },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x0000ffff,0x00000000,0xffff0000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0xffffffff,0x00000000,0xfffe0000,0xfffe0000 }
    };

    uniform unsigned int32 base = seed & PRBS_SEED_MASK;  /* only need the lower 23 bits of the seed value */

    return ((base & masks[0][index])
            ^ (prbs_rotate_left(base, 1) & masks[1][index])
            ^ (prbs_rotate_left(base, 2) & masks[2][index])
            ^ (prbs_rotate_left(base, 3) & masks[3][index])
            ^ (prbs_rotate_left(base, 4) & masks[4][index])
            ^ (prbs_rotate_left(base, 5) & masks[5][index])
            ^ (prbs_rotate_left(base, 6) & masks[6][index])
            ^ (prbs_rotate_left(base, 7) & masks[7][index])
            ^ (prbs_rotate_left(base, 8) & masks[8][index])
            ^ (prbs_rotate_left(base, 9) & masks[9][index])
            ^ (prbs_rotate_left(base, 10) & masks[10][index])
            ^ (prbs_rotate_left(base, 11) & masks[11][index])
            ^ (prbs_rotate_left(base, 12) & masks[12][index])
            ^ (prbs_rotate_left(base, 13) & masks[13][index])
            ^ (prbs_rotate_left(base, 14) & masks[14][index])
            ^ (prbs_rotate_left(base, 15) & masks[15][index])
            ^ (prbs_rotate_left(base, 16) & masks[16][index])
            ^ (prbs_rotate_left(base, 17) & masks[17][index])
            ^ (prbs_rotate_left(base, 18) & masks[18][index])
            ^ (prbs_rotate_left(base, 19) & masks[19][index])
            ^ (prbs_rotate_left(base, 20) & masks[20][index])
            ^ (prbs_rotate_left(base, 21) & masks[21][index])
            ^ (prbs_rotate_left(base, 22) & masks[22][index]));
}

inline unsigned int32 prbs_step(unsigned int32 seed)
{
    static const uniform unsigned int32 masks[] = {
        0x0003ffff,
        0x000fffff,
        0xffffffff,
        0x003fffff,
        0x00000003,
        0x03fffff8,
        0x0000000f,
        0xffffff00,
        0xffffffff,
        0xffffe000,
        0xffffffff,
        0xfffe0000
    };

    unsigned int32 base = seed & PRBS_SEED_MASK;  /* only need the lower 23 bits of the seed value */

    return ((base & masks[0])
            ^ (prbs_rotate_left(base,  2) & masks[1])
            ^ (prbs_rotate_left(base,  3) & masks[2])
            ^ (prbs_rotate_left(base,  4) & masks[3])
            ^ (prbs_rotate_left(base,  7) & masks[4])
            ^ (prbs_rotate_left(base,  8) & masks[5])
            ^ (prbs_rotate_left(base,  9) & masks[6])
            ^ (prbs_rotate_left(base, 13) & masks[7])
            ^ (prbs_rotate_left(base, 17) & masks[8])
            ^ (prbs_rotate_left(base, 18) & masks[9])
            ^ (prbs_rotate_left(base, 20) & masks[10])
            ^ (prbs_rotate_left(base, 22) & masks[11]));
}

#elif TARGET_WIDTH == 16

inline unsigned int32 prbs_next(uniform unsigned int32 seed, int index)
{
    static const uniform unsigned int32 masks[][16] = {
        { 0x00000000,0x0003ffff,0x00000000,0x0003ffff,0x00000000,0x00000000,0x00000000,0x0003ffff,
          0x0003ffff,0x0003ffff,0x0003ffff,0x00000000,0x0003ffff,0x00000000,0x00000000,0x0003ffff },
        { 0x00000000,0x00000000,0xffffffff,0x00000000,0xfff80000,0x00000000,0x00000000,0x00000000,
          0x00000000,0x00000000,0x00000000,0xffffffff,0x00000000,0xfff80000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x000fffff,0x00000000,0xffffffff,0x00000000,0x000fffff,
          0x00000000,0xfff00000,0x00000000,0x00000000,0xfff00000,0x00000000,0xfff00000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0xffe00000,0xffffffff,
          0xffffffff,0xffe00000,0x00000000,0x00000000,0x00000000,0xffe00000,0x00000000,0x001fffff },
        { 0x00000000,0x00000000,0xffffffff,0x00000000,0x003fffff,0x00000000,0x003fffff,0x003fffff,
          0x00000000,0x00000000,0xffc00000,0xffffffff,0x00000000,0x003fffff,0x003fffff,0x003fffff },
        { 0x00000000,0xffffffff,0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,
          0x00000000,0xffffffff,0xffffffff,0x00000000,0x00000000,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00fffffe,0x00000000,0xff000000,0x00000000,0x00000000,0x00000000,
          0xffffffff,0x00000000,0x00000000,0x00fffffe,0x00000000,0xff000000,0x00000000,0xffffffff },
        { 0x00000000,0x00000000,0x00000000,0x00000003,0x00000000,0xfe000003,0x00000000,0x00000003,
          0x00000000,0xfe000000,0xffffffff,0x00000000,0xfe000000,0x00000000,0xfe000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0xffffffff,0x03ffffff,0x03fffff8,
          0x03fffff8,0x03ffffff,0x00000000,0x00000000,0x00000000,0x03ffffff,0xffffffff,0x00000007 },
        { 0xffffffff,0x00000000,0xf800000f,0x00000000,0xfffffff0,0x00000000,0x0000000f,0x0000000f,
          0xffffffff,0xffffffff,0xf8000000,0xf800000f,0x00000000,0xfffffff0,0x0000000f,0x0000000f },
        { 0x00000000,0x0fffffe0,0x00000000,0xffffffff,0x00000000,0x00000000,0x00000000,0x00000000,
          0x00000000,0x0fffffe0,0x0fffffe0,0x00000000,0xffffffff,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x0000003f,0x00000000,0xe0000000,0x00000000,0xffffffff,0x00000000,
          0xe000003f,0x00000000,0x00000000,0x0000003f,0x00000000,0x1fffffff,0x00000000,0x1fffffc0 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0x3fffffff,0x00000000,0x00000000,
          0xffffffff,0xc0000000,0x3fffff80,0x00000000,0x3fffffff,0x00000000,0x3fffffff,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0xffffffff,0x00000000,0x800000ff,0x000000ff,0xffffff00,
          0xffffff00,0x000000ff,0x00000000,0x00000000,0xffffffff,0x000000ff,0x800000ff,0xffffffff },
        { 0xfffffe00,0x00000000,0x00000000,0x00000000,0xfffffe00,0x00000000,0x00000000,0x00000000,
          0xfffffe00,0xfffffe00,0x00000000,0x00000000,0x00000000,0xfffffe00,0x00000000,0x00000000 },
        { 0x00000000,0x000003ff,0x00000000,0x000003ff,0x00000000,0x00000000,0x00000000,0x00000000,
          0x00000000,0xfffffc00,0x000003ff,0x00000000,0x000003ff,0x00000000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0xffffffff,0x00000000,0xfffff800,0x00000000,
          0xffffffff,0x00000000,0x00000000,0xffffffff,0x00000000,0x000007ff,0x00000000,0x000007ff },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x00000000,0x00000fff,0xffffffff,0xffffffff,
          0x00000fff,0x00000000,0xfffff000,0x00000000,0x00000fff,0x00000000,0xfffff000,0xffffffff },
        { 0x00000000,0xffffffff,0x00000000,0xffffe000,0x00000000,0x00000000,0x00000000,0xffffe000,
          0xffffe000,0xffffffff,0xffffffff,0x00000000,0xffffe000,0x00000000,0x00000000,0xffffe000 },
        { 0x00003fff,0x00000000,0x00000000,0x00000000,0xffffc000,0x00000000,0x00000000,0x00000000,
          0x00003fff,0x00003fff,0x00000000,0x00000000,0x00000000,0xffffc000,0x00000000,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0xffffffff,0x00000000,0x00000000,0x00000000,0xffffffff,
          0x00000000,0xffff8000,0x00000000,0x00000000,0xffffffff,0x00000000,0xffffffff,0x00000000 },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0x0000ffff,0x00000000,0xffff0000,0x00000000,
          0x0000ffff,0xffffffff,0x00000000,0x0000ffff,0x00000000,0xffffffff,0x00000000,0xffffffff },
        { 0x00000000,0x00000000,0x00000000,0x00000000,0xffffffff,0x00000000,0xfffe0000,0xfffe0000,
          0x00000000,0x00000000,0xfffe0000,0x00000000,0x00000000,0xffffffff,0xfffe0000,0xfffe0000 }
    };

    uniform unsigned int32 base = seed & PRBS_SEED_MASK;  /* only need the lower 23 bits of the seed value */

    return ((base & masks[0][index])
            ^ (prbs_rotate_left(base, 1) & masks[1][index])
            ^ (prbs_rotate_left(base, 2) & masks[2][index])
            ^ (prbs_rotate_left(base, 3) & masks[3][index])
            ^ (prbs_rotate_left(base, 4) & masks[4][index])
            ^ (prbs_rotate_left(base, 5) & masks[5][index])
            ^ (prbs_rotate_left(base, 6) & masks[6][index])
            ^ (prbs_rotate_left(base, 7) & masks[7][index])
            ^ (prbs_rotate_left(base, 8) & masks[8][index])
            ^ (prbs_rotate_left(base, 9) & masks[9][index])
            ^ (prbs_rotate_left(base, 10) & masks[10][index])
            ^ (prbs_rotate_left(base, 11) & masks[11][index])
            ^ (prbs_rotate_left(base, 12) & masks[12][index])
            ^ (prbs_rotate_left(base, 13) & masks[13][index])
            ^ (prbs_rotate_left(base, 14) & masks[14][index])
            ^ (prbs_rotate_left(base, 15) & masks[15][index])
            ^ (prbs_rotate_left(base, 16) & masks[16][index])
            ^ (prbs_rotate_left(base, 17) & masks[17][index])
            ^ (prbs_rotate_left(base, 18) & masks[18][index])
            ^ (prbs_rotate_left(base, 19) & masks[19][index])
            ^ (prbs_rotate_left(base, 20) & masks[20][index])
            ^ (prbs_rotate_left(base, 21) & masks[21][index])
            ^ (prbs_rotate_left(base, 22) & masks[22][index]));
}

inline unsigned int32 prbs_step(unsigned int32 seed)
{
    static const uniform unsigned int32 masks[] = {
        0x0003ffff,
        0x001fffff,
        0x003fffff,
        0xffffffff,
        0x00000007,
        0x0000000f,
        0x1fffffc0,
        0xffffffff,
        0x000007ff,
        0xffffffff,
        0xffffe000,
        0xffffffff,
        0xfffe0000,
    };

    unsigned int32 base = seed & PRBS_SEED_MASK;  /* only need the lower 23 bits of the seed value */

    return ((base & masks[0])
            ^ (prbs_rotate_left(base,  3) & masks[1])
            ^ (prbs_rotate_left(base,  4) & masks[2])
            ^ (prbs_rotate_left(base,  6) & masks[3])
            ^ (prbs_rotate_left(base,  8) & masks[4])
            ^ (prbs_rotate_left(base,  9) & masks[5])
            ^ (prbs_rotate_left(base, 11) & masks[6])
            ^ (prbs_rotate_left(base, 13) & masks[7])
            ^ (prbs_rotate_left(base, 16) & masks[8])
            ^ (prbs_rotate_left(base, 17) & masks[9])
            ^ (prbs_rotate_left(base, 18) & masks[10])
            ^ (prbs_rotate_left(base, 21) & masks[11])
            ^ (prbs_rotate_left(base, 22) & masks[12]));
}

#endif

/**
 * The n_th chunk of PRBS data consists of the complement of the
 * (n - 1)_th chunk's seed.  Hence, we need to generate the actual PRBS data
 * using the last seed from the previous iteration and the first (lane - 1)
 * seeds of the current iteration.
 */
inline unsigned int32 to_payload(unsigned int32 current, unsigned int32 next)
{
    return (~(shuffle((int32)current, (int32)next,
                      (programIndex == 0
                       ? programCount - 1
                       : programCount + programIndex - 1))));
}


export uniform unsigned int32 fill_prbs_aligned(uniform unsigned int32 payload[],
                                                uniform unsigned int16 length,
                                                uniform unsigned int32 seed)
{
    /* Use the initial seed to generate the appropriate seed for each lane. */
    unsigned int32 curr_seed = seed;
    unsigned int32 next_seed = prbs_next(seed, programIndex);

    uniform int base = 0;
    foreach (i = 0 ... length) {
        /*
         * Blend the current and next lane-wide seed values to create the
         * payload data.
         */
        base += packed_store_active(&payload[base], htonl(to_payload(curr_seed, next_seed)));
        curr_seed = next_seed;
        next_seed = prbs_step(next_seed);
    }

    /* The last active lane contains the seed we need to return. */
    return (extract(curr_seed, (length - 1) & (programCount -1)));
}

/*
 * Note: ISPC provides access to the x86 popcnt instruction via it's standard
 * library function, popcnt().  However, that hardware instruction can only
 * process one lane at a time, so it compiles into a popcount and extract
 * for each lane in use!  This implementation is a 32 bit version of the
 * "Efficient implementation" described in the wikipedia Hamming weight entry,
 * https://en.wikipedia.org/wiki/Hamming_weight#Efficient_implementation,
 * and ends up being faster for our use case.
 */
inline unsigned int32 popcount32(unsigned int32 value)
{
    value = value - ((value >> 1) & 0x55555555);
    value = (value & 0x33333333) + ((value >> 2) & 0x33333333);
    value = (value + (value >> 4) & 0x0f0f0f0f);
    return ((value * 0x01010101) >> 24);
}

export uniform unsigned int64 verify_prbs_aligned(const uniform unsigned int32 payload[],
                                                  uniform unsigned int16 length,
                                                  uniform unsigned int32 expected)
{
    unsigned int32 bit_errors = 0;

    uniform bool prbs_sync = false;
    unsigned int32 curr_seed = expected;
    unsigned int32 next_seed = prbs_next(expected, programIndex);
    uniform int offset = 0;

    /* Attempt to sync to the payload data */
    do {
        int index = offset + programIndex;
        cif (index < length) {
            /* XOR the expected payload with our actual payload to determine bit errors */
            int loop_errors = popcount32(to_payload(curr_seed, next_seed) ^ ntohl(payload[index]));
            bit_errors += loop_errors;

            offset += programCount;

            /*
             * We assume we have found the proper PRBS sequence when
             * our payload matches the expected data for all lanes.
             *
             * However, if we haven't found a matching PRBS pattern
             * yet continue to use the payload as the seed
             * value. Otherwise, crank the matching seed through the
             * PRBS machinery to get the next values.
             */
            if (!(prbs_sync = none(loop_errors))) {
                uniform int base = reduce_max(index) + 1;
                if (base < length) {
                    curr_seed = ~ntohl(payload[base]);
                    next_seed = prbs_next(~ntohl(payload[base]), programIndex);
                }
            } else {
                curr_seed = next_seed;
                next_seed = prbs_step(next_seed);
            }
        }
    } while (!prbs_sync && offset < length);

    /* Once we are synced, we can blast through the remaining payload */
    foreach (i = offset ... length ) {
        bit_errors += popcount32(to_payload(curr_seed, next_seed) ^ ntohl(payload[i]));
        curr_seed = next_seed;
        next_seed = prbs_step(next_seed);
    }

    /* The last active lane contains the seed we need to return. */
    uniform unsigned int64 next_expected = extract(curr_seed, (length - 1) & (programCount - 1));
    return (next_expected << 32 | reduce_add(bit_errors));
}
